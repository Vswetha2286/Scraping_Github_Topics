{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Scraping Top Repositories for Topics on GitHub"
      ],
      "metadata": {
        "id": "W8bV6HUKccvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introduction:\n",
        "\n",
        "Web scraping is the process of extracting and parsing data from websites in an automated fashion using a computer program. It's a useful technique for creating datasets for research and learning.\n",
        "\n",
        "Problem Statement:\n",
        "\n",
        "Getting the list of Topics and scraping the top repositories for each topic on Github.\n",
        "\n",
        "Tools Used:\n",
        "Python,requests,BeautifulSoup,Pandas\n"
      ],
      "metadata": {
        "id": "ZzhxZ8zEc8Dv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the steps to follow:\n",
        "\n",
        "- I'm going to scrape https://github.com/topics.\n",
        "- Parse the downloaded html content using Beautiful Soup.\n",
        "- Will get a list of topics. For each topic, we'll get topic title, topic page\n",
        "  URL and topic description from Soup Object.\n",
        "- For each topic, we'll get the top 25 repositories in the topic from the topic\n",
        "  page\n",
        "- For each repository, we'll grab the repo name, username, stars and repo URL\n",
        "- Creating a DataFrame using pandas libraries of the scraped data\n",
        "- Finally save the dataframe as a CSV file .\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "wf3mB9KDeAAq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Scrape the list of topics from Github"
      ],
      "metadata": {
        "id": "fyK7faqvejtz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries\n",
        "\n",
        "- Used requests library to downlaod the web page.\n",
        "- used BeautifulSoup(BS4) to parse and extract information.\n",
        "- convert to a Pandas dataframe"
      ],
      "metadata": {
        "id": "Y1c2920l2wcO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "yiD2JpBA2tW0"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### URL of the webpage"
      ],
      "metadata": {
        "id": "feYM2TPA3PjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url=\"https://github.com/topics\"\n"
      ],
      "metadata": {
        "id": "f_bDXvVK3U6p"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Sending a get request using the python requests library and we are getting the html page as the response to the request sent"
      ],
      "metadata": {
        "id": "SZlbYSooRFcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response=requests.get(url)\n",
        "response.status_code # Status code 200 means that the request was successful\n"
      ],
      "metadata": {
        "id": "tzVblbJI3ub7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a74c6387-b583-4a76-f14d-3874cc789378"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting the HTML content from the downloaded page"
      ],
      "metadata": {
        "id": "AK1dVhd1Rq-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "page_contents=response.text\n",
        "len(page_contents)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zb1EjlXlRUDV",
        "outputId": "f66c009e-911d-4d89-9643-c83a757d907a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "156401"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parsing the Content into HTML using BeautifulSoup"
      ],
      "metadata": {
        "id": "KcQ-qckSSEuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc=BeautifulSoup(page_contents,'html.parser')\n",
        "type(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjuCviDnR4JI",
        "outputId": "975ed015-35b9-44fc-8ed6-75fb2d9d58bb"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "bs4.BeautifulSoup"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting Topic_Title"
      ],
      "metadata": {
        "id": "6uI06lv8X7fM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selection_class= \"f3 lh-condensed mb-0 mt-1 Link--primary\"\n",
        "topic_title_tags=doc.find_all('p',{'class':selection_class})\n",
        "#topic_title_tags"
      ],
      "metadata": {
        "id": "XwuvWgK9YXZ0"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_titles=[]\n",
        "for tag in topic_title_tags:\n",
        "    topic_titles.append(tag.text)\n",
        "\n",
        "print(topic_titles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f9ed2ac-c376-4786-d6d0-99728647bfcd",
        "id": "AQBujiUEYXZ2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['3D', 'Ajax', 'Algorithm', 'Amp', 'Android', 'Angular', 'Ansible', 'API', 'Arduino', 'ASP.NET', 'Atom', 'Awesome Lists', 'Amazon Web Services', 'Azure', 'Babel', 'Bash', 'Bitcoin', 'Bootstrap', 'Bot', 'C', 'Chrome', 'Chrome extension', 'Command line interface', 'Clojure', 'Code quality', 'Code review', 'Compiler', 'Continuous integration', 'COVID-19', 'C++']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting Topic_Description"
      ],
      "metadata": {
        "id": "WH3tmzP0YXZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "desc_selector=\"f5 color-fg-muted mb-0 mt-1\"\n",
        "topic_desc_tags=doc.find_all('p',{'class': desc_selector})\n",
        "#topic_desc_tags"
      ],
      "metadata": {
        "id": "cfcy_IRxYXZ6"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_desc= []\n",
        "\n",
        "for tag in topic_desc_tags:\n",
        "    topic_desc.append(tag.text.strip())#strip() removes the empty space from the beginning and at the ending\n",
        "\n",
        "print(topic_desc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78f9af00-6259-4502-a7f5-e9d882c8985a",
        "id": "IotgxZsDYXZ7"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['3D refers to the use of three-dimensional graphics, modeling, and animation in various industries.', 'Ajax is a technique for creating interactive web applications.', 'Algorithms are self-contained sequences that carry out a variety of tasks.', 'Amp is a non-blocking concurrency library for PHP.', 'Android is an operating system built by Google designed for mobile devices.', 'Angular is an open source web application platform.', 'Ansible is a simple and powerful automation engine.', 'An API (Application Programming Interface) is a collection of protocols and subroutines for building software.', 'Arduino is an open source platform for building electronic devices.', 'ASP.NET is a web framework for building modern web apps and services.', 'Atom is a open source text editor built with web technologies.', 'An awesome list is a list of awesome things curated by the community.', 'Amazon Web Services provides on-demand cloud computing platforms on a subscription basis.', 'Azure is a cloud computing service created by Microsoft.', 'Babel is a compiler for writing next generation JavaScript, today.', 'Bash is a shell and command language interpreter for the GNU operating system.', 'Bitcoin is a cryptocurrency developed by Satoshi Nakamoto.', 'Bootstrap is an HTML, CSS, and JavaScript framework.', 'A bot is an application that runs automated tasks over the Internet.', 'C is a general purpose programming language that first appeared in 1972.', 'Chrome is a web browser from the tech company Google.', 'Chrome extensions enable users to customize the Chrome browsing experience.', 'A CLI, or command-line interface, is a console that helps users issue commands to a program.', 'Clojure is a dynamic, general-purpose programming language.', 'Automate your code review with style, quality, security, and test‑coverage checks when you need them.', 'Ensure your code meets quality standards and ship with confidence.', 'Compilers are software that translate higher-level programming languages to lower-level languages (e.g. machine code).', 'Automatically build and test your code as you push it upstream, preventing bugs from being deployed to production.', 'The coronavirus disease 2019 (COVID-19) is an infectious disease caused by SARS-CoV-2.', 'C++ is a general purpose and object-oriented programming language.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting Topic_URL"
      ],
      "metadata": {
        "id": "bwQNy69hYXZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_link_tags=doc.find_all('a',{'class':\"no-underline flex-grow-0\"})\n",
        "topic0_url=\"https://github.com\"+ topic_link_tags[0]['href']\n",
        "#print(topic0_url)\n",
        "#topic_link_tags"
      ],
      "metadata": {
        "id": "b93ihfyxYXZ_"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_url=[]\n",
        "base_url= 'https://github.com'\n",
        "\n",
        "for tag in topic_link_tags:\n",
        "    topic_url.append(base_url+ tag['href'])\n",
        "\n",
        "print(topic_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c1adafa-d11d-4c34-d433-fef164ed62df",
        "id": "3Ath6W3CYXaA"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['https://github.com/topics/3d', 'https://github.com/topics/ajax', 'https://github.com/topics/algorithm', 'https://github.com/topics/amphp', 'https://github.com/topics/android', 'https://github.com/topics/angular', 'https://github.com/topics/ansible', 'https://github.com/topics/api', 'https://github.com/topics/arduino', 'https://github.com/topics/aspnet', 'https://github.com/topics/atom', 'https://github.com/topics/awesome', 'https://github.com/topics/aws', 'https://github.com/topics/azure', 'https://github.com/topics/babel', 'https://github.com/topics/bash', 'https://github.com/topics/bitcoin', 'https://github.com/topics/bootstrap', 'https://github.com/topics/bot', 'https://github.com/topics/c', 'https://github.com/topics/chrome', 'https://github.com/topics/chrome-extension', 'https://github.com/topics/cli', 'https://github.com/topics/clojure', 'https://github.com/topics/code-quality', 'https://github.com/topics/code-review', 'https://github.com/topics/compiler', 'https://github.com/topics/continuous-integration', 'https://github.com/topics/covid-19', 'https://github.com/topics/cpp']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Dataframe For the Topic Lists"
      ],
      "metadata": {
        "id": "h9B00mCCYXaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_dict={\n",
        "    'Title': topic_titles,\n",
        "    'Description': topic_desc,\n",
        "    'URL': topic_url\n",
        "}"
      ],
      "metadata": {
        "id": "DVg11pEyYXaD"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_df=pd.DataFrame(topic_dict)\n",
        "#topic_df"
      ],
      "metadata": {
        "id": "N5LIfBPPYXaF"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create CSV file for the List of Topics"
      ],
      "metadata": {
        "id": "nqF6h0hdYyyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_df.to_csv('Topics.csv',index=None)\n",
        "#pd.read_csv(\"Topics.csv\")"
      ],
      "metadata": {
        "id": "1BuM9B1eYyyD"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Get the Top repositories from a topic page\n"
      ],
      "metadata": {
        "id": "cs6x93nyabl3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting Information from each Topic Page"
      ],
      "metadata": {
        "id": "KTlqMg5labmL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function get_repo_info(h3_tag,star_tag) that seperates username, reponame, repo_url and stars count of the repository from the h3 tag list and star_tag list"
      ],
      "metadata": {
        "id": "Sn_bcZVOShYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_repo_info(h3_tag,star_tag):\n",
        "    #returns all the required repository information\n",
        "    a_tags=h3_tag.find_all('a')\n",
        "    username=a_tags[0].text.strip()\n",
        "    repo_name=a_tags[1].text.strip()\n",
        "    repo_url=base_url+a_tags[1]['href']\n",
        "    stars=(star_tag.text.strip())\n",
        "    return username,repo_name,repo_url,stars"
      ],
      "metadata": {
        "id": "oZ0Mq4ntSpkx"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function get_topic_page(topic_url) :                                  - It takes the topic url as an argument and then fetches the content using the\n",
        "  get request of ___requests___ library and then parse the content using BeautifulSoup and returns the same parsed object."
      ],
      "metadata": {
        "id": "xnOaF_rRTMHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_page(topic_url):\n",
        "    response = requests.get(topic_url)\n",
        "    if response.status_code !=200:\n",
        "        raise Exception('Failed to load page {}'.format(topic_url))\n",
        "\n",
        "    topic_doc = BeautifulSoup(response.text,'html.parser')\n",
        "    return topic_doc"
      ],
      "metadata": {
        "id": "Ngmy-n10TUQD"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function get_topic_repos(topic_doc): It takes a Beautiful Soup Object as an argument and find all the h3 tags that contains information about the User name, Repository Name, Repo Url, Stars, stores them in a dictionary and return a object created by converting the dictionary to a Pandas DataFrame"
      ],
      "metadata": {
        "id": "sDG6EdV2Tf4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_repos(topic_doc):\n",
        "\n",
        "    repo_tags = topic_doc.find_all('h3',attrs={\n",
        "        'class' : 'f3 color-fg-muted text-normal lh-condensed'})\n",
        "    stars_tags= topic_doc.find_all('span',attrs= {\n",
        "        'class' : 'Counter js-social-count'\n",
        "            })\n",
        "    topic_repo_dict = {\n",
        "    'Username':[],\n",
        "    'Repo_Name':[],\n",
        "    'Repo_Url':[],\n",
        "    'Stars': []\n",
        "    }\n",
        "    for i in range(len(repo_tags)):\n",
        "        repo_info = get_repo_info(repo_tags[i],stars_tags[i])\n",
        "        topic_repo_dict['Username'].append(repo_info[0])\n",
        "        topic_repo_dict['Repo_Name'].append(repo_info[1])\n",
        "        topic_repo_dict['Repo_Url'].append(repo_info[2])\n",
        "        topic_repo_dict['Stars'].append(repo_info[3])\n",
        "\n",
        "    return pd.DataFrame(topic_repo_dict)\n"
      ],
      "metadata": {
        "id": "vXWQRyEVTLCz"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function scrape_topic(topic_url,topic_name): It scrapes the Top Repositories from the topic url and saves the scraped data as a dataframe to a .csv file, having the file name of the title."
      ],
      "metadata": {
        "id": "JpFxwEiUUGEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_topic(topic_url,topic_name):\n",
        "    filename = topic_name+\".csv\"\n",
        "    if os.path.exists(filename):\n",
        "        print(f\"File {filename} already exists. Skipping...\")\n",
        "        return\n",
        "    topic_df = get_topic_repos(get_topic_page(topic_url))\n",
        "\n",
        "    topic_df.to_csv(filename,index = None)"
      ],
      "metadata": {
        "id": "ju4moM95UH2A"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function get_topic_titles(soup): It takes a Beautiful Soup object and finds all the topic titles present in all the paragraph(p) tags of the html page and returns the same's list"
      ],
      "metadata": {
        "id": "7VwqlfMWURj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_titles(soup):\n",
        "    topic_title_tags = soup.find_all('p',attrs={\n",
        "    'class':'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
        "    })\n",
        "    topic_titles = []\n",
        "\n",
        "    for tag in topic_title_tags:\n",
        "        topic_titles.append(tag.text)\n",
        "    return topic_titles"
      ],
      "metadata": {
        "id": "aH-9HQ1mUW5w"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function get_topic_description(soup): It takes a Beautiful Soup object and finds all the descriptions present in all the paragraph(p) tags of the html page and returns the same's list"
      ],
      "metadata": {
        "id": "g9WZ3-F3Uh5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_description(soup):\n",
        "    topic_desc_tag = soup.find_all('p', attrs=\n",
        "                                   {'class':'f5 color-fg-muted mb-0 mt-1'})\n",
        "    topic_descriptions = []\n",
        "\n",
        "    for tag in topic_desc_tag:\n",
        "        topic_descriptions.append(tag.text.strip())\n",
        "    return topic_descriptions"
      ],
      "metadata": {
        "id": "wfx48LX7UjUT"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function get_topic_urls(soup): It takes a Beautiful Soup object and finds all the links present in all the anchor tags of the html page and returns the same list"
      ],
      "metadata": {
        "id": "TOs7qgULUnNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_urls(soup):\n",
        "    topic_link_tags = soup.find_all('a',attrs =\n",
        "                            {'class' : 'no-underline flex-grow-0'})\n",
        "    topic_urls = []\n",
        "    base = \"https://www.github.com\"\n",
        "    page =\"?page=1\"\n",
        "    for tag in topic_link_tags:\n",
        "        topic_urls.append(base+tag['href'])\n",
        "    return topic_urls"
      ],
      "metadata": {
        "id": "fGtIxx-IUqlT"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function scrape_topics(): scrapes the topic title from the page and their corresponding description and url and returns a DataFrame object."
      ],
      "metadata": {
        "id": "se_t5LK-U1bV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_topics():\n",
        "    topics_url = \"https://github.com/topics\"\n",
        "#     topics_url = \"https://github.com/topics?page=1\"\n",
        "    response = requests.get(topics_url)\n",
        "    if response.status_code !=200:\n",
        "        raise Exception('Failed to load page {}'.format(topic_url))\n",
        "    soup = BeautifulSoup(response.text,'html.parser')\n",
        "    topics_dict = {\n",
        "        'title' : get_topic_titles(soup),\n",
        "        'decscription': get_topic_description(soup),\n",
        "        'url':get_topic_urls(soup)\n",
        "    }\n",
        "    return pd.DataFrame(topics_dict)"
      ],
      "metadata": {
        "id": "zYvxeRusU5el"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bringing all the functions and their activities under one function scrape_topic_repos(): Which first scrapes the topic title from the page and their corresponding description and url, using the function scrape_topics() and then using the url scrapes the top repositories of that particular topic."
      ],
      "metadata": {
        "id": "ejHD0R70U-VH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_topics_repos():\n",
        "    print(\"Scraping list topics from Github\")\n",
        "    topics_df = scrape_topics()\n",
        "    for index,row in topics_df.iterrows():\n",
        "        print(f\"Scraping top repositories for {row['title']}\")\n",
        "        scrape_topic(row['url'],row['title'])"
      ],
      "metadata": {
        "id": "y8X5q6K5UhkJ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scrape_topics_repos()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q91WfPn1VFTC",
        "outputId": "e91c2d45-6151-4476-8fe2-1d9b67db41c9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping list topics from Github\n",
            "Scraping top repositories for 3D\n",
            "Scraping top repositories for Ajax\n",
            "Scraping top repositories for Algorithm\n",
            "Scraping top repositories for Amp\n",
            "Scraping top repositories for Android\n",
            "Scraping top repositories for Angular\n",
            "Scraping top repositories for Ansible\n",
            "Scraping top repositories for API\n",
            "Scraping top repositories for Arduino\n",
            "Scraping top repositories for ASP.NET\n",
            "Scraping top repositories for Atom\n",
            "Scraping top repositories for Awesome Lists\n",
            "Scraping top repositories for Amazon Web Services\n",
            "Scraping top repositories for Azure\n",
            "Scraping top repositories for Babel\n",
            "Scraping top repositories for Bash\n",
            "Scraping top repositories for Bitcoin\n",
            "Scraping top repositories for Bootstrap\n",
            "Scraping top repositories for Bot\n",
            "Scraping top repositories for C\n",
            "Scraping top repositories for Chrome\n",
            "Scraping top repositories for Chrome extension\n",
            "Scraping top repositories for Command line interface\n",
            "Scraping top repositories for Clojure\n",
            "Scraping top repositories for Code quality\n",
            "Scraping top repositories for Code review\n",
            "Scraping top repositories for Compiler\n",
            "Scraping top repositories for Continuous integration\n",
            "Scraping top repositories for COVID-19\n",
            "Scraping top repositories for C++\n"
          ]
        }
      ]
    }
  ]
}